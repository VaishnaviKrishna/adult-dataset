{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8JeyE5V0C-gR"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFUON5gHDAgs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "data=pd.read_csv('drive/My Drive/adult_train_SMALLER.csv')\n",
    "data.replace(' ?', np.NaN,inplace=True)     #Replacing all the missing values with NaN\n",
    "\n",
    "data.columns = [\n",
    "    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n",
    "    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n",
    "    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"]   #for ease of human interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BchwDezHTdyV"
   },
   "outputs": [],
   "source": [
    "#To show that the same information in Education is encoded in integer form in EducationNum\n",
    "#Hence, we drop the categorical column- education\n",
    "\n",
    "plot_educationnum=round(pd.crosstab(data.EducationNum,data.Income).div(pd.crosstab(data.EducationNum,data.Income).apply(sum,1),axis=0),2)\n",
    "plot_educationnum.sort_values(by = ' >50K',inplace=True)\n",
    "ax=plot_educationnum.plot(kind='bar',title='Distribution of income across various education levels',figsize=(10,8))\n",
    "\n",
    "plot_education=round(pd.crosstab(data.Education,data.Income).div(pd.crosstab(data.Education,data.Income).apply(sum,1),axis=0),2)\n",
    "plot_education.sort_values(by = ' >50K',inplace=True)\n",
    "ax=plot_education.plot(kind='bar',title='Distribution of income across various education levels',figsize=(10,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KY0FKtZJDAoz"
   },
   "outputs": [],
   "source": [
    "training_data=data.copy()\n",
    "# There are very less number of rows that contain missing columns. Hence we can safely adopt the below shown method.\n",
    "training_data.dropna(axis=0,how='any',inplace=True)   #Dropping all the missing values (hence reduced training set)\n",
    "training_label=training_data[\"Income\"].map({' <=50K':0,' >50K':1})  #just to give binary labels\n",
    "training_data.drop([\"Income\"],axis=1,inplace=True)\n",
    "#training_data.shape\n",
    "#data[\"Income\"].unique()\n",
    "training_data.drop([\"Education\"],axis=1,inplace=True)   #since it is a redundant feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCnI02vbVJXE"
   },
   "outputs": [],
   "source": [
    "#Data visualization. Native Country versus the income. This can be repeated on all other features to understand\n",
    "#what set of features help the model best in making the right predictions\n",
    "plot_nativecountry=round(pd.crosstab(training_data.NativeCountry,data.Income).div(pd.crosstab(training_data.NativeCountry,data.Income).apply(sum,1),axis=0),2)\n",
    "plot_nativecountry.sort_values(by = ' >50K',inplace=True)\n",
    "ax=plot_nativecountry.plot(kind='bar',title='Distribution of income across Native Country',figsize=(10,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SO3CHRBKVJgO"
   },
   "outputs": [],
   "source": [
    "training_data.NativeCountry.value_counts(normalize=True) * 100  #Since we have 91.22% of US category, we understand that it's variance is very low and so we bin it into two groups\n",
    "#One US bin and the other non-US bin\n",
    "training_data['NativeCountry'] = [' United States' if i == ' United-States'  else ' Out of United States' for i in training_data['NativeCountry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wL4qEqOcKNIg"
   },
   "outputs": [],
   "source": [
    "#Data normalization on numerical columns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Fitting only on training data. We then use the same mean and variance to normalize the test data.\n",
    "scaler.fit(training_data.select_dtypes(\"int64\"))  \n",
    "train_data = scaler.transform(training_data.select_dtypes(\"int64\"))  \n",
    "train_data=pd.DataFrame(train_data)\n",
    "train_data.columns = [\n",
    "    \"Age\", \"fnlwgt\", \"EducationNum\",\n",
    "    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\"]   #for ease of human interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1KWtmLQQKNEt"
   },
   "outputs": [],
   "source": [
    "train_data.set_index(training_data.index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0m6SBt87KNBL"
   },
   "outputs": [],
   "source": [
    "# Data Preparation using One hot encoding\n",
    "\n",
    "training_data_categorical = pd.get_dummies(training_data.select_dtypes('object'))\n",
    "training_data_categorical=training_data_categorical.astype(dtype='category')\n",
    "training_data_non_categorical = train_data                                           #training_data.select_dtypes(exclude = 'object')\n",
    "\n",
    "training_data_onehotencoded = pd.concat([training_data_non_categorical, training_data_categorical], axis=1,join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OuBbxJHVKM85"
   },
   "outputs": [],
   "source": [
    "#Feature selection: Select K best features\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "kbest_selector=SelectKBest(k=training_data_onehotencoded.shape[1])\n",
    "\n",
    "training_selected_features=kbest_selector.fit_transform(training_data_onehotencoded,training_label)\n",
    "\n",
    "selected_cols = kbest_selector.get_support(indices=True)\n",
    "selected_feature_names = training_data_onehotencoded.columns.values[selected_cols]\n",
    "\n",
    "training_selected_features=pd.DataFrame(training_selected_features)\n",
    "\n",
    "scores = kbest_selector.scores_[kbest_selector.get_support()]\n",
    "selected_feature_names_scores = list(zip(selected_feature_names, scores))\n",
    "\n",
    "Feat_F1score_combined = pd.DataFrame(data = selected_feature_names_scores, columns=['Feature_names', 'F_Scores'])\n",
    "Feat_F1score_combined = Feat_F1score_combined.sort_values(['F_Scores', 'Feature_names'], ascending = [False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "duCMeowFU5VI"
   },
   "outputs": [],
   "source": [
    "Feat_F1score_combined.plot(x='Feature_names',y='F_Scores',kind='bar',title='Fscores of features arranged in accordance with their importance using SelectKBest method',figsize=(18,8))\n",
    "#Setting the F score threshold as 30, we get a total of 30 features which have F scores beyond this value\n",
    "\n",
    "kbest_selector=SelectKBest(k=30)\n",
    "training_selected_features=kbest_selector.fit_transform(training_data_onehotencoded,training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mLxKHVNlgERw"
   },
   "outputs": [],
   "source": [
    "#Converting the dataframes to numpy array format\n",
    "train_data_array=np.asarray(training_selected_features)\n",
    "training_label_array=np.asarray(training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owfsTZ--vgsB"
   },
   "outputs": [],
   "source": [
    "# This portion of the code can be used when we want to do dimensionality reduction using PCA\n",
    "'''\n",
    "# Performing PCA\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(training_selected_features)\n",
    "\n",
    "training_features_transformed = pca.transform(training_selected_features)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))   # As can be seen we take 30 components which captures almost all the variance\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative variance')\n",
    "plt.grid('True')\n",
    "'''\n",
    "\n",
    "'''\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.95)   #Select those many components that capture 95% of the variance \n",
    "pca.fit(training_selected_features)\n",
    "\n",
    "training_features_transformed = pca.transform(training_selected_features)\n",
    "\n",
    "train_data_array=np.asarray(training_features_transformed)\n",
    "training_label_array=np.asarray(training_label)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cgaTFecuKjV5"
   },
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('drive/My Drive/adult_test_SMALLER.csv')\n",
    "test_data.replace(' ?', np.NaN,inplace=True)\n",
    "\n",
    "test_data.columns = [\n",
    "    \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n",
    "    \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n",
    "    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"]   #for ease of human interpretation\n",
    "testing_data=test_data\n",
    "testing_data.dropna(axis=0,how='any',inplace=True)\n",
    "testing_label=testing_data[\"Income\"].map({' <=50K.':0,' >50K.':1})  #just to give binary labels\n",
    "testing_data.drop([\"Income\"],axis=1,inplace=True)\n",
    "\n",
    "testing_data.drop([\"Education\"],axis=1,inplace=True)   #since it is a redundant feature\n",
    "\n",
    "# Applying same transformation to test data for normalization\n",
    "\n",
    "test_data = scaler.transform(testing_data.select_dtypes(\"int64\"))\n",
    "test_data=pd.DataFrame(test_data)\n",
    "test_data.columns = [\n",
    "    \"Age\", \"fnlwgt\", \"EducationNum\",\n",
    "    \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\"]   #for ease of human interpretation\n",
    "\n",
    "test_data.set_index(testing_data.index,inplace=True)\n",
    "\n",
    "testing_data['NativeCountry'] = [' United States' if i == ' United-States'  else ' Out of United States' for i in testing_data['NativeCountry']]\n",
    "\n",
    "# Data Prep using One hot encoding (on test data)\n",
    "\n",
    "testing_data_categorical = pd.get_dummies(testing_data.select_dtypes('object'))\n",
    "testing_data_categorical=testing_data_categorical.astype(dtype='category')\n",
    "testing_data_non_categorical =   test_data                                         #testing_data.select_dtypes(exclude = 'object')\n",
    "\n",
    "testing_data_onehotencoded = pd.concat([testing_data_non_categorical, testing_data_categorical], axis=1,join='inner')\n",
    "\n",
    "\n",
    "#Matching the #of columns in the training data post one hot encoding\n",
    "\n",
    "missing_test_col_set=set(training_data_onehotencoded.columns.values.tolist()).difference(testing_data_onehotencoded.columns.values.tolist())\n",
    "#list(missing_test_col_set)\n",
    "for i in range(len(list(missing_test_col_set))):\n",
    "    testing_data_onehotencoded.loc[ : , list(missing_test_col_set)[i]] = 0\n",
    "    \n",
    "testing_selected_features=kbest_selector.transform(testing_data_onehotencoded)\n",
    "# Below line of the code can be used when we want to do dimensionality reduction using PCA\n",
    "#testing_features_transformed = pca.transform(testing_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x_BrycaGKM5W"
   },
   "outputs": [],
   "source": [
    "#Finding the optimal hyperparameters for SVM with linear kernel using cross validation\n",
    "import sklearn                         \n",
    "from sklearn import svm\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "fold = model_selection.StratifiedKFold(n_splits = 5,shuffle = True)\n",
    "C_val = np.logspace(-2,1,num=10)\n",
    "gamma_val = np.logspace(-2,1,num=10)\n",
    "size_gamma = np.size(gamma_val)\n",
    "size_c = np.size(C_val)\n",
    "optimal_gamma = -10 \n",
    "optimal_C = -10\n",
    "acc_max = -100\n",
    "\n",
    "\n",
    "for i in range(0,size_gamma):\n",
    "    for j in range(0,size_c):\n",
    "        current_gamma = gamma_val[i]\n",
    "        print(current_gamma)\n",
    "        current_C = C_val[j]\n",
    "        print(current_C)\n",
    "        temp=[]\n",
    "        for tr_idx,val_idx in fold.split(train_data_array,training_label_array):\n",
    "            X_train, X_val= train_data_array[tr_idx],train_data_array[val_idx]\n",
    "            y_train, y_val= training_label_array[tr_idx],training_label_array[val_idx]\n",
    "            svm_clf=svm.SVC(gamma=current_gamma,C=current_C,kernel='linear')\n",
    "            svm_clf.fit(X_train,y_train)\n",
    "            y_pred=svm_clf.predict(X_val)\n",
    "            acc_val = metrics.accuracy_score(y_val,y_pred)\n",
    "            print(acc_val)\n",
    "            temp.append(acc_val)\n",
    "            \n",
    "        if  mean(temp) > acc_max:\n",
    "            acc_max = mean(temp)\n",
    "            optimal_gamma = gamma_val[i]\n",
    "            optimal_C = C_val[j]\n",
    "print(\"Training Accuracy : \",acc_max)\n",
    "print(\"Optimal Hyper parameters : gamma : \",optimal_gamma,\" C:\",optimal_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tnqeg_w7i6rw"
   },
   "outputs": [],
   "source": [
    "#Testing using penalized SVM classifier with linear kernel\n",
    "\n",
    "svm_clf=svm.SVC(kernel='linear',C=optimal_C,gamma=optimal_gamma,class_weight='balanced')\n",
    "svm_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = svm_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6m7siP0-Tnta"
   },
   "outputs": [],
   "source": [
    "#Testing using SVM classifier with linear kernel\n",
    "\n",
    "svm_clf=svm.SVC(kernel='linear',C=optimal_C,gamma=optimal_gamma)\n",
    "svm_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = svm_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKr5vRdZSH2D"
   },
   "outputs": [],
   "source": [
    "# To visualize the performance difference between normal and weighted(penalized) SVM with linear kernel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# we create two clusters of random points\n",
    "n_samples_1 = training_label.value_counts().iloc[0]\n",
    "n_samples_2 = training_label.value_counts().iloc[1]\n",
    "centers = [[0.0, 0.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 0.5]\n",
    "X, y = make_blobs(n_samples=[n_samples_1, n_samples_2],\n",
    "                  centers=centers,\n",
    "                  cluster_std=clusters_std,\n",
    "                  random_state=0, shuffle=False)\n",
    "\n",
    "# fit the model and get the separating hyperplane\n",
    "clf = svm.SVC(kernel='linear', C=optimal_C,gamma=optimal_gamma)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# fit the model and get the separating hyperplane using weighted classes\n",
    "wclf = svm.SVC(kernel='linear', class_weight='balanced',C=optimal_C,gamma=optimal_gamma)\n",
    "wclf.fit(X, y)\n",
    "\n",
    "# plot the samples\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "\n",
    "# plot the decision functions for both classifiers\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "# get the separating hyperplane\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "a = ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])\n",
    "\n",
    "# get the separating hyperplane for weighted classes\n",
    "Z = wclf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins for weighted classes\n",
    "b = ax.contour(XX, YY, Z, colors='r', levels=[0], alpha=0.5, linestyles=['-'])\n",
    "\n",
    "plt.legend([a.collections[0], b.collections[0]], [\"non weighted\", \"weighted\"],\n",
    "           loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "#This portion of the code has been taken from the scikit-learn documnetation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5o0Vqi8KMyV"
   },
   "outputs": [],
   "source": [
    "#Finding the optimal hyperparameters for SVM with rbf kernel using cross validation\n",
    "import sklearn                          \n",
    "from sklearn import svm\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "fold = model_selection.StratifiedKFold(n_splits = 5,shuffle = True)\n",
    "C_val = np.logspace(-2,1,num=10)\n",
    "gamma_val = np.logspace(-2,1,num=10)\n",
    "size_gamma = np.size(gamma_val)\n",
    "size_c = np.size(C_val)\n",
    "optimal_gamma = -10 \n",
    "optimal_C = -10\n",
    "acc_max = -100\n",
    "\n",
    "\n",
    "for i in range(0,size_gamma):\n",
    "    for j in range(0,size_c):\n",
    "        current_gamma = gamma_val[i]\n",
    "        print(current_gamma)\n",
    "        current_C = C_val[j]\n",
    "        print(current_C)\n",
    "        temp=[]\n",
    "        for tr_idx,val_idx in fold.split(train_data_array,training_label_array):\n",
    "            X_train, X_val= train_data_array[tr_idx],train_data_array[val_idx]\n",
    "            y_train, y_val= training_label_array[tr_idx],training_label_array[val_idx]\n",
    "            svm_clf=svm.SVC(gamma=current_gamma,C=current_C,kernel='rbf')\n",
    "            svm_clf.fit(X_train,y_train)\n",
    "            y_pred=svm_clf.predict(X_val)\n",
    "            acc_val = metrics.accuracy_score(y_val,y_pred)\n",
    "            print(acc_val)\n",
    "            temp.append(acc_val)\n",
    "            \n",
    "        if  mean(temp) > acc_max:\n",
    "            acc_max = mean(temp)\n",
    "            optimal_gamma = gamma_val[i]\n",
    "            optimal_C = C_val[j]\n",
    "print(\"Training Accuracy : \",acc_max)\n",
    "print(\"Optimal Hyper parameters : gamma : \",optimal_gamma,\" C:\",optimal_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WPEgjFzzKMux"
   },
   "outputs": [],
   "source": [
    "#Testing using SVM classifier with gaussian kernel\n",
    "\n",
    "svm_clf=svm.SVC(kernel='rbf',C=optimal_C,gamma=optimal_gamma)\n",
    "svm_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = svm_clf.predict(testing_selected_features)\n",
    "\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U0ZfByLbphft"
   },
   "outputs": [],
   "source": [
    "#Testing using weighted(penalized) SVM classifier with gaussian kernel \n",
    "\n",
    "svm_clf=svm.SVC(kernel='rbf',C=optimal_C,gamma=optimal_gamma,class_weight='balanced')\n",
    "svm_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = svm_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aSA_qRg-bRWk"
   },
   "outputs": [],
   "source": [
    "# To visualize the performance difference between normal and weighted(penalized) SVM with gaussian (rbf) kernel\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# we create two clusters of random points\n",
    "n_samples_1 = training_label.value_counts().iloc[0]\n",
    "n_samples_2 = training_label.value_counts().iloc[1]\n",
    "centers = [[0.0, 0.0], [2.0, 2.0]]\n",
    "clusters_std = [1.5, 0.5]\n",
    "X, y = make_blobs(n_samples=[n_samples_1, n_samples_2],\n",
    "                  centers=centers,\n",
    "                  cluster_std=clusters_std,\n",
    "                  random_state=0, shuffle=False)\n",
    "\n",
    "# fit the model and get the separating hyperplane\n",
    "clf = svm.SVC(kernel='rbf', C=optimal_C,gamma=optimal_gamma)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# fit the model and get the separating hyperplane using weighted classes\n",
    "wclf = svm.SVC(kernel='rbf', class_weight='balanced',C=optimal_C,gamma=optimal_gamma)\n",
    "wclf.fit(X, y)\n",
    "\n",
    "# plot the samples\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "\n",
    "# plot the decision functions for both classifiers\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# create grid to evaluate model\n",
    "xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "YY, XX = np.meshgrid(yy, xx)\n",
    "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "\n",
    "# get the separating hyperplane\n",
    "Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins\n",
    "a = ax.contour(XX, YY, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])\n",
    "\n",
    "# get the separating hyperplane for weighted classes\n",
    "Z = wclf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "# plot decision boundary and margins for weighted classes\n",
    "b = ax.contour(XX, YY, Z, colors='y', levels=[0], alpha=0.5, linestyles=['-'])\n",
    "\n",
    "plt.legend([a.collections[0], b.collections[0]], [\"non weighted\", \"weighted\"],\n",
    "           loc=\"upper right\")\n",
    "plt.show()\n",
    "\n",
    "#This portion of the code has been taken from the scikit-learn documnetation examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SoDN1XdjKMrf"
   },
   "outputs": [],
   "source": [
    "#FInd best parameters for SVM with 'polynomial' kernel using cross validation \n",
    "import sklearn                          \n",
    "from sklearn import svm\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "fold = model_selection.StratifiedKFold(n_splits = 5,shuffle = True)\n",
    "C_val = np.logspace(-2,1,num=10)\n",
    "gamma_val = np.logspace(-2,1,num=10)\n",
    "size_gamma = np.size(gamma_val)\n",
    "size_c = np.size(C_val)\n",
    "optimal_gamma = -10 \n",
    "optimal_C = -10\n",
    "acc_max = -100\n",
    "\n",
    "\n",
    "for i in range(0,size_gamma):\n",
    "    for j in range(0,size_c):\n",
    "        current_gamma = gamma_val[i]\n",
    "        print(current_gamma)\n",
    "        current_C = C_val[j]\n",
    "        print(current_C)\n",
    "        temp=[]\n",
    "        for tr_idx,val_idx in fold.split(train_data_array,training_label_array):\n",
    "            X_train, X_val= train_data_array[tr_idx],train_data_array[val_idx]\n",
    "            y_train, y_val= training_label_array[tr_idx],training_label_array[val_idx]\n",
    "            svm_clf=svm.SVC(gamma=current_gamma,C=current_C,kernel='poly')\n",
    "            svm_clf.fit(X_train,y_train)\n",
    "            y_pred=svm_clf.predict(X_val)\n",
    "            acc_val = metrics.accuracy_score(y_val,y_pred)\n",
    "            print(acc_val)\n",
    "            temp.append(acc_val)\n",
    "            \n",
    "        if  mean(temp) > acc_max:\n",
    "            acc_max = mean(temp)\n",
    "            optimal_gamma = gamma_val[i]\n",
    "            optimal_C = C_val[j]\n",
    "print(\"Training Accuracy : \",acc_max)\n",
    "print(\"Optimal Hyper parameters : gamma : \",optimal_gamma,\" C:\",optimal_C)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wv-FADMdKMnk"
   },
   "outputs": [],
   "source": [
    "#Testing using SVM classifier with polynomial kernel\n",
    "\n",
    "svm_clf=svm.SVC(kernel='poly',C=optimal_C,gamma=optimal_gamma)\n",
    "svm_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = svm_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V93GZyk9KMjl"
   },
   "outputs": [],
   "source": [
    "#The cross validation part on reduced dataset\n",
    "import sklearn                          \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "#FInd best parameters for MLP using cross validation \n",
    "\n",
    "folds = model_selection.StratifiedKFold(n_splits = 5,shuffle = True)\n",
    "hidden_layers_val = np.arange(1,100)\n",
    "acc_max = -1000\n",
    "hidden_layer_opt = -10\n",
    "\n",
    "for i in hidden_layers_val:\n",
    "    print(i)\n",
    "    temp=[]\n",
    "    for tr_ind,v_ind in folds.split(train_data_array,training_label_array):\n",
    "        X_train, X_val = train_data_array[tr_ind],train_data_array[v_ind]\n",
    "        y_train, y_val = training_label_array[tr_ind],training_label_array[v_ind]\n",
    "        mlp_clf = MLPClassifier(hidden_layer_sizes=i,max_iter=1000)\n",
    "        mlp_clf.fit(X_train,y_train)\n",
    "        y_pred = mlp_clf.predict(X_val)\n",
    "        acc_val = metrics.accuracy_score(y_val,y_pred)\n",
    "        print(acc_val)\n",
    "        temp.append(acc_val)\n",
    "    if mean(temp) > acc_max:\n",
    "        acc_max = mean(temp)\n",
    "        hidden_layer_opt = i\n",
    "                         \n",
    "print(\"Training Accuracy : \",acc_max)                    \n",
    "print(\"Optimal Value : Hidden layer size : \",hidden_layer_opt)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y_b8l71KMer"
   },
   "outputs": [],
   "source": [
    "#Using the optimal parameters to train and test the MLP Classifier\n",
    "\n",
    "import sklearn                          \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "hidden_layer_opt=18\n",
    "#Testing using MLP classifier \n",
    "\n",
    "mlp_clf=MLPClassifier(hidden_layer_sizes=hidden_layer_opt)\n",
    "mlp_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = mlp_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NnC7ycfWObNB"
   },
   "outputs": [],
   "source": [
    "#Using Naive Bayes (Gaussian) classifier \n",
    "\n",
    "import sklearn                          \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "nb_clf=GaussianNB()\n",
    "nb_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = nb_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kkaI09qiObet"
   },
   "outputs": [],
   "source": [
    "#FInd best parameters for Decision Tree using cross validation \n",
    "\n",
    "import sklearn                          \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "folds = model_selection.StratifiedKFold(n_splits = 5,shuffle = True)\n",
    "depth_val = np.arange(1,100)\n",
    "acc_max = -1000\n",
    "opt_dep = -10\n",
    "for i in range(0,np.size(depth_val)):\n",
    "        temp=[]\n",
    "       # print(i)\n",
    "       # n_trial = depth_val[i]\n",
    "        for tr_ind,v_ind in folds.split(train_data_array,training_label_array):\n",
    "            X_train, X_val = train_data_array[tr_ind],train_data_array[v_ind]\n",
    "            y_train, y_val = training_label_array[tr_ind],training_label_array[v_ind]\n",
    "            dectree_clf = DecisionTreeClassifier(max_depth = depth_val[i])\n",
    "            dectree_clf.fit(X_train,y_train)\n",
    "            y_pred = dectree_clf.predict(X_val)\n",
    "            acc_val = metrics.accuracy_score(y_val,y_pred)\n",
    "            temp.append(acc_val)\n",
    "            if mean(temp) > acc_max:\n",
    "                acc_max = mean(temp)\n",
    "                opt_dep = depth_val[i]\n",
    "print(\"Training Accuracy : \",acc_max)                    \n",
    "print(\"Optimal Value : Max depth : \",opt_dep)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctZ40RPhQ8Tf"
   },
   "outputs": [],
   "source": [
    "#Decision Tree on test data\n",
    "\n",
    "dectree_clf=DecisionTreeClassifier(max_depth = opt_dep)\n",
    "dectree_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = dectree_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlPo9u-JPH2s"
   },
   "outputs": [],
   "source": [
    "#FInd best parameters for Random Forest using cross validation \n",
    "\n",
    "import sklearn                          \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "folds = model_selection.StratifiedKFold(n_splits = 5,shuffle = True)\n",
    "estimators_num = np.arange(1,100)\n",
    "acc_max = -1000\n",
    "opt_est = -10\n",
    "for i in range(0,np.size(estimators_num)):\n",
    "        temp=[]\n",
    "        #n_trial = g[i]\n",
    "        for tr_ind,v_ind in folds.split(train_data_array,training_label_array):\n",
    "            X_train, X_val = train_data_array[tr_ind],train_data_array[v_ind]\n",
    "            y_train, y_val = training_label_array[tr_ind],training_label_array[v_ind]\n",
    "            rand_forest_clf = RandomForestClassifier(n_estimators = estimators_num[i])\n",
    "            rand_forest_clf.fit(X_train,y_train)\n",
    "            y_pred = rand_forest_clf.predict(X_val)\n",
    "            acc_val = metrics.accuracy_score(y_val,y_pred)\n",
    "            temp.append(acc_val)\n",
    "            if mean(temp) > acc_max:\n",
    "                acc_max = mean(temp)\n",
    "                opt_est = estimators_num[i]\n",
    "print(\"Training Accuracy : \",acc_max)                    \n",
    "print(\"Optimal Value : No.of estimators : \",opt_est)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XMGnGYy-PHxF"
   },
   "outputs": [],
   "source": [
    "#Random Forest on test data\n",
    "\n",
    "rand_forest_clf=RandomForestClassifier(n_estimators = opt_est)\n",
    "rand_forest_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = rand_forest_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGAcb5aCPHrS"
   },
   "outputs": [],
   "source": [
    "#FInd best parameters for KNN using cross validation \n",
    "\n",
    "import sklearn                          \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import model_selection,metrics\n",
    "from statistics import mean\n",
    "\n",
    "folds = model_selection.StratifiedKFold(n_splits = 5,shuffle = True)\n",
    "neighbors_num = np.arange(1,25)\n",
    "acc_max = -1000\n",
    "neighbors_opt = -10\n",
    "for i in range(0,np.size(neighbors_num)):\n",
    "        #n_trial = g[i]\n",
    "        temp=[]\n",
    "        for tr_ind,v_ind in folds.split(train_data_array,training_label_array):\n",
    "            X_train, X_val = train_data_array[tr_ind],train_data_array[v_ind]\n",
    "            y_train, y_val = training_label_array[tr_ind],training_label_array[v_ind]\n",
    "            knn_clf = KNeighborsClassifier(n_neighbors = neighbors_num[i])\n",
    "            knn_clf.fit(X_train,y_train)\n",
    "            y_pred = knn_clf.predict(X_val)\n",
    "            acc_val = metrics.accuracy_score(y_val,y_pred)\n",
    "            temp.append(acc_val)\n",
    "            if mean(temp) > acc_max:\n",
    "                acc_max = mean(temp)\n",
    "                neighbors_opt = neighbors_num[i]\n",
    "                        \n",
    "print(\"Optimal Value : No.of neighbors : \",neighbors_opt)\n",
    "print(\"Training Accuracy : \",acc_max)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RO-vyZPtPHmU"
   },
   "outputs": [],
   "source": [
    "#KNN on test data\n",
    "\n",
    "knn_clf=KNeighborsClassifier(n_neighbors = neighbors_opt)\n",
    "knn_clf.fit(training_selected_features,training_label)\n",
    "\n",
    "predicted_label = knn_clf.predict(testing_selected_features)\n",
    "testing_accuracy=metrics.accuracy_score(testing_label,predicted_label)\n",
    "print(testing_accuracy)\n",
    "#report_an=metrics.precision_recall_fscore_support(testing_label,predicted_label,average='binary')  \n",
    "#print(report_an)\n",
    "report=metrics.classification_report(testing_label,predicted_label)\n",
    "print(report)\n",
    "\n",
    "auc_score=metrics.roc_auc_score(testing_label,predicted_label,average='macro')\n",
    "print(auc_score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Trial_1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
